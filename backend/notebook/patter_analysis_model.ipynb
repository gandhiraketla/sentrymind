{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b10af8-3d16-46f6-bf03-1b6354edee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "700446c5-c984-4727-89bf-24a46ca03fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>2019-01-01 12:47:15</td>\n",
       "      <td>60416207185</td>\n",
       "      <td>fraud_Jones, Sawayn and Romaguera</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>7.27</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Diaz</td>\n",
       "      <td>F</td>\n",
       "      <td>9886 Anita Drive</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0048</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>Information systems manager</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>98e3dcf98101146a577f85a34e58feec</td>\n",
       "      <td>1325422035</td>\n",
       "      <td>43.974711</td>\n",
       "      <td>-109.741904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>2724</td>\n",
       "      <td>2019-01-02 08:44:57</td>\n",
       "      <td>60416207185</td>\n",
       "      <td>fraud_Berge LLC</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>52.94</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Diaz</td>\n",
       "      <td>F</td>\n",
       "      <td>9886 Anita Drive</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0048</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>Information systems manager</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>498120fc45d277f7c88e3dba79c33865</td>\n",
       "      <td>1325493897</td>\n",
       "      <td>42.018766</td>\n",
       "      <td>-109.044172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>2726</td>\n",
       "      <td>2019-01-02 08:47:36</td>\n",
       "      <td>60416207185</td>\n",
       "      <td>fraud_Luettgen PLC</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>82.08</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Diaz</td>\n",
       "      <td>F</td>\n",
       "      <td>9886 Anita Drive</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0048</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>Information systems manager</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>95f514bb993151347c7acdf8505c3d62</td>\n",
       "      <td>1325494056</td>\n",
       "      <td>42.961335</td>\n",
       "      <td>-109.157564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>2882</td>\n",
       "      <td>2019-01-02 12:38:14</td>\n",
       "      <td>60416207185</td>\n",
       "      <td>fraud_Daugherty LLC</td>\n",
       "      <td>kids_pets</td>\n",
       "      <td>34.79</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Diaz</td>\n",
       "      <td>F</td>\n",
       "      <td>9886 Anita Drive</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0048</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>Information systems manager</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>4f0c1a14e0aa7eb56a490780ef9268c5</td>\n",
       "      <td>1325507894</td>\n",
       "      <td>42.228227</td>\n",
       "      <td>-108.747683</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>2907</td>\n",
       "      <td>2019-01-02 13:10:46</td>\n",
       "      <td>60416207185</td>\n",
       "      <td>fraud_Beier and Sons</td>\n",
       "      <td>home</td>\n",
       "      <td>27.18</td>\n",
       "      <td>Mary</td>\n",
       "      <td>Diaz</td>\n",
       "      <td>F</td>\n",
       "      <td>9886 Anita Drive</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0048</td>\n",
       "      <td>-108.8964</td>\n",
       "      <td>1645</td>\n",
       "      <td>Information systems manager</td>\n",
       "      <td>1986-02-17</td>\n",
       "      <td>3b2ebd3af508afba959640893e1e82bc</td>\n",
       "      <td>1325509846</td>\n",
       "      <td>43.321745</td>\n",
       "      <td>-108.091143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 trans_date_trans_time       cc_num  \\\n",
       "1017        1017   2019-01-01 12:47:15  60416207185   \n",
       "2724        2724   2019-01-02 08:44:57  60416207185   \n",
       "2726        2726   2019-01-02 08:47:36  60416207185   \n",
       "2882        2882   2019-01-02 12:38:14  60416207185   \n",
       "2907        2907   2019-01-02 13:10:46  60416207185   \n",
       "\n",
       "                               merchant       category    amt first  last  \\\n",
       "1017  fraud_Jones, Sawayn and Romaguera       misc_net   7.27  Mary  Diaz   \n",
       "2724                    fraud_Berge LLC  gas_transport  52.94  Mary  Diaz   \n",
       "2726                 fraud_Luettgen PLC  gas_transport  82.08  Mary  Diaz   \n",
       "2882                fraud_Daugherty LLC      kids_pets  34.79  Mary  Diaz   \n",
       "2907               fraud_Beier and Sons           home  27.18  Mary  Diaz   \n",
       "\n",
       "     gender            street  ...      lat      long  city_pop  \\\n",
       "1017      F  9886 Anita Drive  ...  43.0048 -108.8964      1645   \n",
       "2724      F  9886 Anita Drive  ...  43.0048 -108.8964      1645   \n",
       "2726      F  9886 Anita Drive  ...  43.0048 -108.8964      1645   \n",
       "2882      F  9886 Anita Drive  ...  43.0048 -108.8964      1645   \n",
       "2907      F  9886 Anita Drive  ...  43.0048 -108.8964      1645   \n",
       "\n",
       "                              job         dob  \\\n",
       "1017  Information systems manager  1986-02-17   \n",
       "2724  Information systems manager  1986-02-17   \n",
       "2726  Information systems manager  1986-02-17   \n",
       "2882  Information systems manager  1986-02-17   \n",
       "2907  Information systems manager  1986-02-17   \n",
       "\n",
       "                             trans_num   unix_time  merch_lat  merch_long  \\\n",
       "1017  98e3dcf98101146a577f85a34e58feec  1325422035  43.974711 -109.741904   \n",
       "2724  498120fc45d277f7c88e3dba79c33865  1325493897  42.018766 -109.044172   \n",
       "2726  95f514bb993151347c7acdf8505c3d62  1325494056  42.961335 -109.157564   \n",
       "2882  4f0c1a14e0aa7eb56a490780ef9268c5  1325507894  42.228227 -108.747683   \n",
       "2907  3b2ebd3af508afba959640893e1e82bc  1325509846  43.321745 -108.091143   \n",
       "\n",
       "      is_fraud  \n",
       "1017         0  \n",
       "2724         0  \n",
       "2726         0  \n",
       "2882         0  \n",
       "2907         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:\\\\sentrymind\\\\backend\\\\data\\\\fraudTrain.csv\")\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "\n",
    "# Sort transactions by customer and time\n",
    "df = df.sort_values(by=['cc_num', 'trans_date_trans_time'])\n",
    "\n",
    "# Verify first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1dfcbbdf-41eb-4e98-baaa-49086677b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transaction_amount'] = df['amt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "25dbadcf-f987-424b-b3cf-2c6ba55e96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume merchant risk scores are precomputed and stored in a dictionary\n",
    "merchant_risk_scores = {\n",
    "    'merchant_1': 7.5, 'merchant_2': 2.3, 'merchant_3': 9.0,  # Example values\n",
    "}\n",
    "df['merchant_risk_score'] = df['merchant'].map(merchant_risk_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c9d09-e33f-44f7-ac6b-a6b4cb475d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fffd9574-7db5-4178-a5f4-042c3896c9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gandh\\AppData\\Local\\Temp\\ipykernel_32428\\3186324961.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['transaction_count_24h'] = df.groupby('cc_num', group_keys=False).apply(count_past_24h_transactions)['trans_num']\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp column to datetime\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "\n",
    "# Sort transactions chronologically within each customer group\n",
    "df = df.sort_values(by=['cc_num', 'trans_date_trans_time'])\n",
    "\n",
    "# âœ… Corrected approach: Use `.apply()` within `groupby`\n",
    "def count_past_24h_transactions(group):\n",
    "    return group.rolling('1d', on='trans_date_trans_time').count()\n",
    "\n",
    "df['transaction_count_24h'] = df.groupby('cc_num', group_keys=False).apply(count_past_24h_transactions)['trans_num']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a8fc5ad8-9500-48a0-958a-440119037abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gandh\\AppData\\Local\\Temp\\ipykernel_32428\\2551260755.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('cc_num', group_keys=False).apply(calculate_avg_spend)\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp column to datetime\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "\n",
    "# Sort transactions chronologically within each customer\n",
    "df = df.sort_values(by=['cc_num', 'trans_date_trans_time'])\n",
    "\n",
    "# Define function to compute 30-day average spend\n",
    "def calculate_avg_spend(group):\n",
    "    group = group.set_index('trans_date_trans_time').sort_index()  # Set index for rolling window\n",
    "    group['avg_spend_last_30d'] = group['amt'].rolling('30D').mean()  # Apply rolling mean\n",
    "    return group.reset_index()  # Reset index back to original format\n",
    "\n",
    "# Apply function while maintaining DataFrame structure\n",
    "df = df.groupby('cc_num', group_keys=False).apply(calculate_avg_spend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f5551adb-39ba-47ae-b4c8-d1bba8c55fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gandh\\AppData\\Local\\Temp\\ipykernel_32428\\362746785.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('cc_num', group_keys=False).apply(max_spend_last_30d)\n"
     ]
    }
   ],
   "source": [
    "def max_spend_last_30d(group):\n",
    "    group = group.set_index('trans_date_trans_time').sort_index()\n",
    "    group['max_spend_last_30d'] = group['amt'].rolling('30D').max()\n",
    "    return group.reset_index()\n",
    "\n",
    "df = df.groupby('cc_num', group_keys=False).apply(max_spend_last_30d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e2453213-267a-4033-bba9-f32ee2f9a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gandh\\AppData\\Local\\Temp\\ipykernel_32428\\1502302882.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('cc_num', group_keys=False).apply(min_spend_last_30d)\n"
     ]
    }
   ],
   "source": [
    "def min_spend_last_30d(group):\n",
    "    group = group.set_index('trans_date_trans_time').sort_index()\n",
    "    group['min_spend_last_30d'] = group['amt'].rolling('30D').min()\n",
    "    return group.reset_index()\n",
    "\n",
    "df = df.groupby('cc_num', group_keys=False).apply(min_spend_last_30d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a676c4d8-44dc-4767-b6d1-ec0dc6b406d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/125.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/125.4 kB ? eta -:--:--\n",
      "   ------------ -------------------------- 41.0/125.4 kB 487.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 125.4/125.4 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.3/40.3 kB ? eta 0:00:00\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~nnxruntime (C:\\Users\\gandh\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~nnxruntime (C:\\Users\\gandh\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a4f0cc48-64b2-4e5e-a3dd-73c92217c869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Computing features...\n",
      "âœ“ 24-hour transaction counts computed\n",
      "âœ“ Rolling statistics computed\n",
      "âœ“ Past fraud counts computed\n",
      "âœ“ Account age computed\n",
      "Computing distances (this may take a few minutes)...\n",
      "Processed 0/1296675 records...\n",
      "Processed 50000/1296675 records...\n",
      "Processed 100000/1296675 records...\n",
      "Processed 150000/1296675 records...\n",
      "Processed 200000/1296675 records...\n",
      "Processed 250000/1296675 records...\n",
      "Processed 300000/1296675 records...\n",
      "Processed 350000/1296675 records...\n",
      "Processed 400000/1296675 records...\n",
      "Processed 450000/1296675 records...\n",
      "Processed 500000/1296675 records...\n",
      "Processed 550000/1296675 records...\n",
      "Processed 600000/1296675 records...\n",
      "Processed 650000/1296675 records...\n",
      "Processed 700000/1296675 records...\n",
      "Processed 750000/1296675 records...\n",
      "Processed 800000/1296675 records...\n",
      "Processed 850000/1296675 records...\n",
      "Processed 900000/1296675 records...\n",
      "Processed 950000/1296675 records...\n",
      "Processed 1000000/1296675 records...\n",
      "Processed 1050000/1296675 records...\n",
      "Processed 1100000/1296675 records...\n",
      "Processed 1150000/1296675 records...\n",
      "Processed 1200000/1296675 records...\n",
      "Processed 1250000/1296675 records...\n",
      "âœ“ Customer-merchant distances computed\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'merchant_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'merchant_state'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Customer-merchant distances computed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# âœ… Compute If Transaction Is in a Different State\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerchant_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ State mismatches computed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Save cleaned dataset\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'merchant_state'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(\"C:\\\\sentrymind\\\\backend\\\\data\\\\fraudTrain.csv\")\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "# Sort transactions chronologically within each customer\n",
    "df = df.sort_values(by=['cc_num', 'trans_date_trans_time'])\n",
    "\n",
    "print(\"Computing features...\")\n",
    "\n",
    "# âœ… Compute Transaction Count in Last 24 Hours\n",
    "def count_past_24h_transactions(group):\n",
    "    try:\n",
    "        group = group.copy()\n",
    "        group = group.set_index('trans_date_trans_time')\n",
    "        group['transaction_count_24h'] = group.rolling('24H', min_periods=1)['trans_num'].count()\n",
    "        return group.reset_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in 24h transactions: {e}\")\n",
    "        group['transaction_count_24h'] = 0\n",
    "        return group\n",
    "\n",
    "df = df.groupby('cc_num', group_keys=False).apply(count_past_24h_transactions)\n",
    "print(\"âœ“ 24-hour transaction counts computed\")\n",
    "\n",
    "# âœ… Compute Rolling Statistics (30-day window)\n",
    "def calculate_rolling_stats(group):\n",
    "    try:\n",
    "        group = group.copy()\n",
    "        group = group.set_index('trans_date_trans_time')\n",
    "        \n",
    "        # Calculate all rolling statistics\n",
    "        rolling_window = group['amt'].rolling('30D', min_periods=1)\n",
    "        group['avg_spend_last_30d'] = rolling_window.mean()\n",
    "        group['max_spend_last_30d'] = rolling_window.max()\n",
    "        group['min_spend_last_30d'] = rolling_window.min()\n",
    "        group['monthly_spend_avg'] = group['avg_spend_last_30d'] * 30\n",
    "        \n",
    "        return group.reset_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in rolling stats: {e}\")\n",
    "        for col in ['avg_spend_last_30d', 'max_spend_last_30d', 'min_spend_last_30d', 'monthly_spend_avg']:\n",
    "            group[col] = np.nan\n",
    "        return group\n",
    "\n",
    "df = df.groupby('cc_num', group_keys=False).apply(calculate_rolling_stats)\n",
    "print(\"âœ“ Rolling statistics computed\")\n",
    "\n",
    "# âœ… Compute Number of Unique Merchants in Last 30 Days\n",
    "# âœ… Compute Number of Unique Merchants in Last 30 Days - Optimized Version\n",
    "def unique_merchants_last_30d(group):\n",
    "    try:\n",
    "        group = group.copy()\n",
    "        group = group.set_index('trans_date_trans_time')\n",
    "        \n",
    "        # Create a helper function to count unique merchants efficiently\n",
    "        def count_unique_in_window(window_data):\n",
    "            if len(window_data) == 0:\n",
    "                return 0\n",
    "            return len(set(window_data))\n",
    "        \n",
    "        # Use rolling apply with the optimized function\n",
    "        group['unique_merchants_30d'] = (\n",
    "            group['merchant']\n",
    "            .rolling('30D', min_periods=1)\n",
    "            .agg(count_unique_in_window)\n",
    "        )\n",
    "        \n",
    "        return group.reset_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in unique merchants: {e}\")\n",
    "        group['unique_merchants_30d'] = 0\n",
    "        return group\n",
    "\n",
    "# âœ… Compute Number of Past Fraud Transactions in Last 30 Days\n",
    "def past_fraud_last_30d(group):\n",
    "    try:\n",
    "        group = group.copy()\n",
    "        group = group.set_index('trans_date_trans_time')\n",
    "        group['past_fraud_30d'] = group['is_fraud'].rolling('30D', min_periods=1).sum()\n",
    "        return group.reset_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in past fraud: {e}\")\n",
    "        group['past_fraud_30d'] = 0\n",
    "        return group\n",
    "\n",
    "df = df.groupby('cc_num', group_keys=False).apply(past_fraud_last_30d)\n",
    "print(\"âœ“ Past fraud counts computed\")\n",
    "\n",
    "# âœ… Compute Account Age in Days\n",
    "df['account_age_days'] = (df['trans_date_trans_time'] - df['dob']).dt.days\n",
    "print(\"âœ“ Account age computed\")\n",
    "\n",
    "# âœ… Compute Distance Between Merchant and Customer\n",
    "# âœ… Compute Distance Between Merchant and Customer - Optimized Version\n",
    "print(\"Computing distances (this may take a few minutes)...\")\n",
    "\n",
    "# Vectorize the distance calculation for better performance\n",
    "def compute_distances_vectorized(df):\n",
    "    try:\n",
    "        # Create masks for valid coordinates\n",
    "        valid_coords = (\n",
    "            df['lat'].notna() & \n",
    "            df['long'].notna() & \n",
    "            df['merch_lat'].notna() & \n",
    "            df['merch_long'].notna()\n",
    "        )\n",
    "        \n",
    "        # Initialize distances array with NaN\n",
    "        distances = np.full(len(df), np.nan)\n",
    "        \n",
    "        # Only compute for valid coordinates\n",
    "        valid_df = df[valid_coords]\n",
    "        \n",
    "        # Process in chunks to avoid memory issues\n",
    "        chunk_size = 10000\n",
    "        for i in range(0, len(valid_df), chunk_size):\n",
    "            chunk = valid_df.iloc[i:i+chunk_size]\n",
    "            chunk_distances = [\n",
    "                geodesic(\n",
    "                    (row['lat'], row['long']),\n",
    "                    (row['merch_lat'], row['merch_long'])\n",
    "                ).miles\n",
    "                for _, row in chunk.iterrows()\n",
    "            ]\n",
    "            distances[valid_df.index[i:i+chunk_size]] = chunk_distances\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 50000 == 0:\n",
    "                print(f\"Processed {i}/{len(valid_df)} records...\")\n",
    "        \n",
    "        return distances\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in distance computation: {e}\")\n",
    "        return np.full(len(df), np.nan)\n",
    "\n",
    "# Compute distances\n",
    "df['customer_merchant_distance'] = compute_distances_vectorized(df)\n",
    "print(\"âœ“ Customer-merchant distances computed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e979b4b7-c3dd-43fb-bd5d-b150a1efd5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in dataset:\n",
      "['trans_date_trans_time', 'Unnamed: 0', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud', 'transaction_count_24h', 'avg_spend_last_30d', 'max_spend_last_30d', 'min_spend_last_30d', 'monthly_spend_avg', 'past_fraud_30d', 'account_age_days', 'customer_merchant_distance']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns in dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "272a9e1f-db6c-494d-9303-60e3d9a36c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_merchants_last_30d_ultrafast(group):\n",
    "    try:\n",
    "        # Downsample to 3-day periods for extreme speed\n",
    "        group = group.copy()\n",
    "        group['date'] = group['trans_date_trans_time'].dt.date\n",
    "        three_day_merchants = (\n",
    "            group.groupby(group['date'].map(lambda x: x - pd.Timedelta(days=x.day % 3)))\n",
    "            ['merchant'].agg(set)\n",
    "        )\n",
    "        group['unique_merchants_30d'] = group['date'].map(\n",
    "            lambda x: len(set().union(*three_day_merchants[\n",
    "                (three_day_merchants.index <= x) & \n",
    "                (three_day_merchants.index > x - pd.Timedelta(days=30))\n",
    "            ]))\n",
    "        )\n",
    "        return group\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        group['unique_merchants_30d'] = 0\n",
    "        return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "59795b9d-c39b-4441-8434-a37fe77c0e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing unique merchants (this should take 5-7 minutes)...\n",
      "âœ“ Unique merchants computed\n",
      "\n",
      "Verifying all features:\n",
      "transaction_count_24h: âœ“\n",
      "avg_spend_last_30d: âœ“\n",
      "max_spend_last_30d: âœ“\n",
      "min_spend_last_30d: âœ“\n",
      "unique_merchants_30d: âœ“\n",
      "past_fraud_30d: âœ“\n",
      "account_age_days: âœ“\n",
      "monthly_spend_avg: âœ“\n",
      "customer_merchant_distance: âœ“\n",
      "\n",
      "Dataset saved with all features including unique_merchants_30d\n"
     ]
    }
   ],
   "source": [
    "# âœ… Compute Number of Unique Merchants in Last 30 Days - Ultra Fast Version\n",
    "def unique_merchants_last_30d_ultrafast(group):\n",
    "    try:\n",
    "        # Downsample to 3-day periods for extreme speed\n",
    "        group = group.copy()\n",
    "        group['date'] = group['trans_date_trans_time'].dt.date\n",
    "        three_day_merchants = (\n",
    "            group.groupby(group['date'].map(lambda x: x - pd.Timedelta(days=x.day % 3)))\n",
    "            ['merchant'].agg(set)\n",
    "        )\n",
    "        group['unique_merchants_30d'] = group['date'].map(\n",
    "            lambda x: len(set().union(*three_day_merchants[\n",
    "                (three_day_merchants.index <= x) & \n",
    "                (three_day_merchants.index > x - pd.Timedelta(days=30))\n",
    "            ]))\n",
    "        )\n",
    "        return group\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        group['unique_merchants_30d'] = 0\n",
    "        return group\n",
    "\n",
    "print(\"Computing unique merchants (this should take 5-7 minutes)...\")\n",
    "df = df.groupby('cc_num', group_keys=False).apply(unique_merchants_last_30d_ultrafast)\n",
    "print(\"âœ“ Unique merchants computed\")\n",
    "\n",
    "# Now let's verify all features again\n",
    "expected_features = [\n",
    "    'transaction_count_24h', \n",
    "    'avg_spend_last_30d', \n",
    "    'max_spend_last_30d',\n",
    "    'min_spend_last_30d', \n",
    "    'unique_merchants_30d',\n",
    "    'past_fraud_30d',\n",
    "    'account_age_days', \n",
    "    'monthly_spend_avg', \n",
    "    'customer_merchant_distance'\n",
    "]\n",
    "\n",
    "print(\"\\nVerifying all features:\")\n",
    "for feature in expected_features:\n",
    "    exists = feature in df.columns\n",
    "    print(f\"{feature}: {'âœ“' if exists else 'âœ—'}\")\n",
    "\n",
    "# If successful, save the updated dataset\n",
    "if 'unique_merchants_30d' in df.columns:\n",
    "    df.to_csv(\"fraud_features_engineered.csv\", index=False)\n",
    "    print(\"\\nDataset saved with all features including unique_merchants_30d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f840d698-6064-41d6-96b5-4806fb84766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading engineered dataset...\n",
      "\n",
      "1. Available columns:\n",
      "['trans_date_trans_time', 'Unnamed: 0', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud', 'transaction_count_24h', 'avg_spend_last_30d', 'max_spend_last_30d', 'min_spend_last_30d', 'monthly_spend_avg', 'past_fraud_30d', 'account_age_days', 'customer_merchant_distance', 'date', 'unique_merchants_30d']\n",
      "\n",
      "2. Missing values in each column:\n",
      "trans_date_trans_time               0\n",
      "Unnamed: 0                          0\n",
      "cc_num                              0\n",
      "merchant                            0\n",
      "category                            0\n",
      "amt                                 0\n",
      "first                               0\n",
      "last                                0\n",
      "gender                              0\n",
      "street                              0\n",
      "city                                0\n",
      "state                               0\n",
      "zip                                 0\n",
      "lat                                 0\n",
      "long                                0\n",
      "city_pop                            0\n",
      "job                                 0\n",
      "dob                                 0\n",
      "trans_num                           0\n",
      "unix_time                           0\n",
      "merch_lat                           0\n",
      "merch_long                          0\n",
      "is_fraud                            0\n",
      "transaction_count_24h               0\n",
      "avg_spend_last_30d                  0\n",
      "max_spend_last_30d                  0\n",
      "min_spend_last_30d                  0\n",
      "monthly_spend_avg                   0\n",
      "past_fraud_30d                      0\n",
      "account_age_days                    0\n",
      "customer_merchant_distance    1293552\n",
      "date                                0\n",
      "unique_merchants_30d                0\n",
      "dtype: int64\n",
      "\n",
      "3. Fraud distribution:\n",
      "is_fraud\n",
      "0    0.994211\n",
      "1    0.005789\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "4. Engineered features statistics:\n",
      "       transaction_count_24h  avg_spend_last_30d  max_spend_last_30d  \\\n",
      "count           1.296675e+06        1.296675e+06        1.296675e+06   \n",
      "mean            4.884108e+00        7.021796e+01        8.255628e+02   \n",
      "std             3.082072e+00        2.881329e+01        1.162141e+03   \n",
      "min             1.000000e+00        1.030000e+00        1.030000e+00   \n",
      "25%             3.000000e+00        5.276395e+01        3.295000e+02   \n",
      "50%             4.000000e+00        6.363100e+01        5.542500e+02   \n",
      "75%             6.000000e+00        8.343406e+01        9.523800e+02   \n",
      "max             3.600000e+01        1.433540e+03        2.894890e+04   \n",
      "\n",
      "       min_spend_last_30d  unique_merchants_30d  past_fraud_30d  \\\n",
      "count        1.296675e+06          1.296675e+06    1.296675e+06   \n",
      "mean         4.000784e+00          9.439052e+01    4.252129e-01   \n",
      "std          1.072011e+01          4.366808e+01    2.075475e+00   \n",
      "min          1.000000e+00          1.000000e+00    0.000000e+00   \n",
      "25%          1.080000e+00          6.200000e+01    0.000000e+00   \n",
      "50%          1.200000e+00          9.200000e+01    0.000000e+00   \n",
      "75%          1.530000e+00          1.230000e+02    0.000000e+00   \n",
      "max          1.433540e+03          2.870000e+02    1.900000e+01   \n",
      "\n",
      "       account_age_days  monthly_spend_avg  customer_merchant_distance  \n",
      "count      1.296675e+06       1.296675e+06                 3123.000000  \n",
      "mean       1.680018e+04       2.106539e+03                   46.282343  \n",
      "std        6.353166e+03       8.643987e+02                   18.087930  \n",
      "min        5.085000e+03       3.090000e+01                    0.597068  \n",
      "25%        1.190700e+04       1.582918e+03                   33.279885  \n",
      "50%        1.606000e+04       1.908930e+03                   47.349989  \n",
      "75%        2.084500e+04       2.503022e+03                   60.051068  \n",
      "max        3.493200e+04       4.300620e+04                   85.139581  \n",
      "\n",
      "5. Number of unique values in categorical features:\n",
      "merchant: 693 unique values\n",
      "category: 14 unique values\n",
      "\n",
      "6. Data types of columns:\n",
      "trans_date_trans_time          object\n",
      "Unnamed: 0                      int64\n",
      "cc_num                          int64\n",
      "merchant                       object\n",
      "category                       object\n",
      "amt                           float64\n",
      "first                          object\n",
      "last                           object\n",
      "gender                         object\n",
      "street                         object\n",
      "city                           object\n",
      "state                          object\n",
      "zip                             int64\n",
      "lat                           float64\n",
      "long                          float64\n",
      "city_pop                        int64\n",
      "job                            object\n",
      "dob                            object\n",
      "trans_num                      object\n",
      "unix_time                       int64\n",
      "merch_lat                     float64\n",
      "merch_long                    float64\n",
      "is_fraud                        int64\n",
      "transaction_count_24h         float64\n",
      "avg_spend_last_30d            float64\n",
      "max_spend_last_30d            float64\n",
      "min_spend_last_30d            float64\n",
      "monthly_spend_avg             float64\n",
      "past_fraud_30d                float64\n",
      "account_age_days                int64\n",
      "customer_merchant_distance    float64\n",
      "date                           object\n",
      "unique_merchants_30d            int64\n",
      "dtype: object\n",
      "\n",
      "7. Checking for anomalies in key features:\n",
      "\n",
      "transaction_count_24h:\n",
      "Min: 1.0\n",
      "Max: 36.0\n",
      "% zeros: 0.0\n",
      "\n",
      "avg_spend_last_30d:\n",
      "Min: 1.03\n",
      "Max: 1433.54\n",
      "% zeros: 0.0\n",
      "\n",
      "max_spend_last_30d:\n",
      "Min: 1.03\n",
      "Max: 28948.9\n",
      "% zeros: 0.0\n",
      "\n",
      "min_spend_last_30d:\n",
      "Min: 1.0\n",
      "Max: 1433.54\n",
      "% zeros: 0.0\n",
      "\n",
      "unique_merchants_30d:\n",
      "Min: 1\n",
      "Max: 287\n",
      "% zeros: 0.0\n",
      "\n",
      "past_fraud_30d:\n",
      "Min: 0.0\n",
      "Max: 19.0\n",
      "% zeros: 95.35134092968555\n",
      "\n",
      "account_age_days:\n",
      "Min: 5085\n",
      "Max: 34932\n",
      "% zeros: 0.0\n",
      "\n",
      "monthly_spend_avg:\n",
      "Min: 30.9\n",
      "Max: 43006.2\n",
      "% zeros: 0.0\n",
      "\n",
      "customer_merchant_distance:\n",
      "Min: 0.5970676429637596\n",
      "Max: 85.13958109238618\n",
      "% zeros: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the engineered dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading engineered dataset...\")\n",
    "df = pd.read_csv(\"fraud_features_engineered.csv\")\n",
    "\n",
    "# 1. Check all columns\n",
    "print(\"\\n1. Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# 2. Check data completeness\n",
    "print(\"\\n2. Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 3. Check target variable distribution\n",
    "print(\"\\n3. Fraud distribution:\")\n",
    "print(df['is_fraud'].value_counts(normalize=True))\n",
    "\n",
    "# 4. Check engineered features statistics\n",
    "engineered_features = [\n",
    "    'transaction_count_24h', \n",
    "    'avg_spend_last_30d', \n",
    "    'max_spend_last_30d',\n",
    "    'min_spend_last_30d', \n",
    "    'unique_merchants_30d',\n",
    "    'past_fraud_30d',\n",
    "    'account_age_days', \n",
    "    'monthly_spend_avg', \n",
    "    'customer_merchant_distance'\n",
    "]\n",
    "\n",
    "print(\"\\n4. Engineered features statistics:\")\n",
    "print(df[engineered_features].describe())\n",
    "\n",
    "# 5. Check categorical features\n",
    "categorical_features = ['merchant', 'category']\n",
    "print(\"\\n5. Number of unique values in categorical features:\")\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# 6. Check data types\n",
    "print(\"\\n6. Data types of columns:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 7. Check for any anomalies in key features\n",
    "print(\"\\n7. Checking for anomalies in key features:\")\n",
    "for feature in engineered_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(\"Min:\", df[feature].min())\n",
    "        print(\"Max:\", df[feature].max())\n",
    "        print(\"% zeros:\", (df[feature] == 0).mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "20d2771e-3197-471a-bde8-4ccc6f424ba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare features\n",
    "features_to_use = [\n",
    "    # Amount-related\n",
    "    'amt', 'avg_spend_last_30d', 'max_spend_last_30d', 'min_spend_last_30d', 'monthly_spend_avg',\n",
    "    \n",
    "    # Transaction patterns\n",
    "    'transaction_count_24h', 'unique_merchants_30d', 'past_fraud_30d',\n",
    "    \n",
    "    # Location/Time\n",
    "    'customer_merchant_distance', 'account_age_days',\n",
    "    \n",
    "    # Categorical\n",
    "    'merchant', 'category'\n",
    "]\n",
    "\n",
    "# 2. Encode categorical variables\n",
    "le_merchant = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "\n",
    "df['merchant_encoded'] = le_merchant.fit_transform(df['merchant'])\n",
    "df['category_encoded'] = le_category.fit_transform(df['category'])\n",
    "\n",
    "# Update features to use encoded versions\n",
    "features_to_use = [f if f not in ['merchant', 'category'] else f+'_encoded' for f in features_to_use]\n",
    "\n",
    "# 3. Split the data\n",
    "X = df[features_to_use].fillna(-999)  # Simple handling of missing values\n",
    "y = df['is_fraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Setup XGBoost with parameters suited for imbalanced data\n",
    "xgb_params = {\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta': 0.3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'scale_pos_weight': len(y_train[y_train==0]) / len(y_train[y_train==1]),  # Handle class imbalance\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'hist'  # For faster training\n",
    "}\n",
    "\n",
    "# 5. Train model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Train with early stopping\n",
    "model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred_proba = model.predict(dtest)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# 7. Print model performance\n",
    "print(\"\\nModel Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "# 8. Feature importance analysis\n",
    "importance = model.get_score(importance_type='gain')\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for feature, score in sorted_importance[:10]:\n",
    "    print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ab363068-7edb-422c-a888-e5e00620914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\gandh\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\gandh\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB 330.3 kB/s eta 0:06:19\n",
      "   ---------------------------------------- 0.1/124.9 MB 751.6 kB/s eta 0:02:47\n",
      "   ---------------------------------------- 0.4/124.9 MB 2.8 MB/s eta 0:00:46\n",
      "   ---------------------------------------- 1.2/124.9 MB 6.1 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 4.5/124.9 MB 17.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 4.8/124.9 MB 17.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 7.2/124.9 MB 21.0 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 8.8/124.9 MB 22.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 13.0/124.9 MB 50.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 17.3/124.9 MB 72.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 22.1/124.9 MB 93.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 25.4/124.9 MB 81.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 30.8/124.9 MB 93.0 MB/s eta 0:00:02\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 35.6/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 37.8/124.9 MB 9.2 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 40.7/124.9 MB 9.1 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 45.0/124.9 MB 9.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 45.4/124.9 MB 8.7 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 50.3/124.9 MB 65.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 51.4/124.9 MB 65.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 52.3/124.9 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 57.2/124.9 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 57.9/124.9 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 60.8/124.9 MB 46.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 63.0/124.9 MB 59.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 68.2/124.9 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 73.4/124.9 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 73.4/124.9 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 73.4/124.9 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 73.4/124.9 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 73.4/124.9 MB 110.0 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 73.4/124.9 MB 110.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 74.0/124.9 MB 28.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 74.0/124.9 MB 28.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 74.3/124.9 MB 22.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.3/124.9 MB 21.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 19.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 76.1/124.9 MB 9.4 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 78.8/124.9 MB 9.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 82.0/124.9 MB 9.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 86.8/124.9 MB 73.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 92.1/124.9 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 97.9/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------ 102.9/124.9 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 107.7/124.9 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 110.1/124.9 MB 93.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.5/124.9 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 118.9/124.9 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  121.7/124.9 MB 93.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.9/124.9 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 34.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~nnxruntime (C:\\Users\\gandh\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~nnxruntime (C:\\Users\\gandh\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b9c72a51-8b42-4166-b601-6c2baac7e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.99525\ttest-auc:0.99517\n",
      "[50]\ttrain-auc:0.99998\ttest-auc:0.99993\n",
      "[100]\ttrain-auc:1.00000\ttest-auc:0.99995\n",
      "[150]\ttrain-auc:1.00000\ttest-auc:0.99995\n",
      "[200]\ttrain-auc:1.00000\ttest-auc:0.99996\n",
      "[250]\ttrain-auc:1.00000\ttest-auc:0.99996\n",
      "[300]\ttrain-auc:1.00000\ttest-auc:0.99996\n",
      "[350]\ttrain-auc:1.00000\ttest-auc:0.99996\n",
      "[372]\ttrain-auc:1.00000\ttest-auc:0.99996\n",
      "\n",
      "Model Performance:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    257834\n",
      "           1       0.94      0.97      0.96      1501\n",
      "\n",
      "    accuracy                           1.00    259335\n",
      "   macro avg       0.97      0.99      0.98    259335\n",
      "weighted avg       1.00      1.00      1.00    259335\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[257740     94]\n",
      " [    40   1461]]\n",
      "\n",
      "ROC AUC Score: 0.9999585151588555\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "past_fraud_30d: 3810.491455078125\n",
      "max_spend_last_30d: 209.3304443359375\n",
      "category_encoded: 140.88954162597656\n",
      "amt: 127.01374816894531\n",
      "transaction_count_24h: 90.78632354736328\n",
      "avg_spend_last_30d: 70.36753845214844\n",
      "unique_merchants_30d: 43.42738342285156\n",
      "monthly_spend_avg: 40.58232879638672\n",
      "account_age_days: 20.05974769592285\n",
      "min_spend_last_30d: 9.915669441223145\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare features\n",
    "features_to_use = [\n",
    "    # Amount-related\n",
    "    'amt', 'avg_spend_last_30d', 'max_spend_last_30d', 'min_spend_last_30d', 'monthly_spend_avg',\n",
    "    \n",
    "    # Transaction patterns\n",
    "    'transaction_count_24h', 'unique_merchants_30d', 'past_fraud_30d',\n",
    "    \n",
    "    # Location/Time\n",
    "    'customer_merchant_distance', 'account_age_days',\n",
    "    \n",
    "    # Categorical\n",
    "    'merchant', 'category'\n",
    "]\n",
    "\n",
    "# 2. Encode categorical variables\n",
    "le_merchant = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "\n",
    "df['merchant_encoded'] = le_merchant.fit_transform(df['merchant'])\n",
    "df['category_encoded'] = le_category.fit_transform(df['category'])\n",
    "\n",
    "# Update features to use encoded versions\n",
    "features_to_use = [f if f not in ['merchant', 'category'] else f+'_encoded' for f in features_to_use]\n",
    "\n",
    "# 3. Split the data\n",
    "X = df[features_to_use].fillna(-999)  # Simple handling of missing values\n",
    "y = df['is_fraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Setup XGBoost with parameters suited for imbalanced data\n",
    "xgb_params = {\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta': 0.3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'scale_pos_weight': len(y_train[y_train==0]) / len(y_train[y_train==1]),  # Handle class imbalance\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'hist'  # For faster training\n",
    "}\n",
    "\n",
    "# 5. Train model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Train with early stopping\n",
    "model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred_proba = model.predict(dtest)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# 7. Print model performance\n",
    "print(\"\\nModel Performance:\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "# 8. Feature importance analysis\n",
    "importance = model.get_score(importance_type='gain')\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for feature, score in sorted_importance[:10]:\n",
    "    print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "25411fdf-b9d9-483f-a700-03bc19350eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\gandh\n",
      "\n",
      "Model saved at: models\\fraud_detection_model.json\n",
      "Merchant encoder saved at: models\\fraud_detection_merchant_encoder.joblib\n",
      "Category encoder saved at: models\\fraud_detection_category_encoder.joblib\n"
     ]
    }
   ],
   "source": [
    "# In your Jupyter notebook, first let's see current directory\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Create a models directory if it doesn't exist\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Now save the model and encoders in the models directory\n",
    "model_path = os.path.join(model_dir, \"fraud_detection_model.json\")\n",
    "merchant_encoder_path = os.path.join(model_dir, \"fraud_detection_merchant_encoder.joblib\")\n",
    "category_encoder_path = os.path.join(model_dir, \"fraud_detection_category_encoder.joblib\")\n",
    "\n",
    "# Save the model and encoders\n",
    "model.save_model(model_path)\n",
    "joblib.dump(le_merchant, merchant_encoder_path)\n",
    "joblib.dump(le_category, category_encoder_path)\n",
    "\n",
    "print(\"\\nModel saved at:\", model_path)\n",
    "print(\"Merchant encoder saved at:\", merchant_encoder_path)\n",
    "print(\"Category encoder saved at:\", category_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e2767-4bad-4032-99ca-b0ed0d7764eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
